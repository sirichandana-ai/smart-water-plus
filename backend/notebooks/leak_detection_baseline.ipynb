{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59a42054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d09940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Mini_Project_4-1\\smart-water-plus\\simulation\\simulation\\datasets\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Mini_Project_4-1\\smart-water-plus\\simulation\\simulation\\datasets\\test.csv\")\n",
    "\n",
    "\n",
    "X_train, y_train = train.drop(columns=['leak', 'timestamp']), train['leak']\n",
    "X_test, y_test = test.drop(columns=['leak', 'timestamp']), test['leak']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7eef058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'pressure', 'flow', 'rolling_mean', 'delta',\n",
      "       'pressure_lag_1', 'flow_lag_1', 'pressure_lag_2', 'flow_lag_2',\n",
      "       'pressure_lag_3', 'flow_lag_3', 'pressure_lag_4', 'flow_lag_4',\n",
      "       'pressure_lag_5', 'flow_lag_5', 'pressure_lag_6', 'flow_lag_6',\n",
      "       'pressure_lag_7', 'flow_lag_7', 'pressure_lag_8', 'flow_lag_8',\n",
      "       'pressure_lag_9', 'flow_lag_9', 'pressure_lag_10', 'flow_lag_10',\n",
      "       'pressure_min', 'pressure_max', 'pressure_std', 'flow_min', 'flow_max',\n",
      "       'flow_std', 'leak'],\n",
      "      dtype='object')\n",
      "Index(['timestamp', 'pressure', 'flow', 'rolling_mean', 'delta',\n",
      "       'pressure_lag_1', 'flow_lag_1', 'pressure_lag_2', 'flow_lag_2',\n",
      "       'pressure_lag_3', 'flow_lag_3', 'pressure_lag_4', 'flow_lag_4',\n",
      "       'pressure_lag_5', 'flow_lag_5', 'pressure_lag_6', 'flow_lag_6',\n",
      "       'pressure_lag_7', 'flow_lag_7', 'pressure_lag_8', 'flow_lag_8',\n",
      "       'pressure_lag_9', 'flow_lag_9', 'pressure_lag_10', 'flow_lag_10',\n",
      "       'pressure_min', 'pressure_max', 'pressure_std', 'flow_min', 'flow_max',\n",
      "       'flow_std', 'leak'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74c8942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9d816a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pressure           2\n",
      "flow               0\n",
      "rolling_mean       2\n",
      "delta              0\n",
      "pressure_lag_1     2\n",
      "flow_lag_1         0\n",
      "pressure_lag_2     2\n",
      "flow_lag_2         0\n",
      "pressure_lag_3     2\n",
      "flow_lag_3         2\n",
      "pressure_lag_4     2\n",
      "flow_lag_4         2\n",
      "pressure_lag_5     2\n",
      "flow_lag_5         2\n",
      "pressure_lag_6     2\n",
      "flow_lag_6         2\n",
      "pressure_lag_7     2\n",
      "flow_lag_7         2\n",
      "pressure_lag_8     2\n",
      "flow_lag_8         2\n",
      "pressure_lag_9     2\n",
      "flow_lag_9         2\n",
      "pressure_lag_10    2\n",
      "flow_lag_10        2\n",
      "pressure_min       2\n",
      "pressure_max       2\n",
      "pressure_std       0\n",
      "flow_min           0\n",
      "flow_max           0\n",
      "flow_std           0\n",
      "dtype: int64\n",
      "pressure           0\n",
      "flow               0\n",
      "rolling_mean       0\n",
      "delta              0\n",
      "pressure_lag_1     1\n",
      "flow_lag_1         0\n",
      "pressure_lag_2     1\n",
      "flow_lag_2         0\n",
      "pressure_lag_3     1\n",
      "flow_lag_3         1\n",
      "pressure_lag_4     1\n",
      "flow_lag_4         1\n",
      "pressure_lag_5     1\n",
      "flow_lag_5         1\n",
      "pressure_lag_6     1\n",
      "flow_lag_6         1\n",
      "pressure_lag_7     1\n",
      "flow_lag_7         1\n",
      "pressure_lag_8     1\n",
      "flow_lag_8         1\n",
      "pressure_lag_9     1\n",
      "flow_lag_9         1\n",
      "pressure_lag_10    1\n",
      "flow_lag_10        1\n",
      "pressure_min       0\n",
      "pressure_max       0\n",
      "pressure_std       0\n",
      "flow_min           0\n",
      "flow_max           0\n",
      "flow_std           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.isnull().sum())\n",
    "print(X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de0a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['pressure' 'rolling_mean' 'pressure_lag_1' 'pressure_lag_2'\n",
      " 'pressure_lag_3' 'flow_lag_3' 'pressure_lag_4' 'flow_lag_4'\n",
      " 'pressure_lag_5' 'flow_lag_5' 'pressure_lag_6' 'flow_lag_6'\n",
      " 'pressure_lag_7' 'flow_lag_7' 'pressure_lag_8' 'flow_lag_8'\n",
      " 'pressure_lag_9' 'flow_lag_9' 'pressure_lag_10' 'flow_lag_10'\n",
      " 'pressure_min' 'pressure_max']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['pressure' 'rolling_mean' 'pressure_lag_1' 'pressure_lag_2'\n",
      " 'pressure_lag_3' 'flow_lag_3' 'pressure_lag_4' 'flow_lag_4'\n",
      " 'pressure_lag_5' 'flow_lag_5' 'pressure_lag_6' 'flow_lag_6'\n",
      " 'pressure_lag_7' 'flow_lag_7' 'pressure_lag_8' 'flow_lag_8'\n",
      " 'pressure_lag_9' 'flow_lag_9' 'pressure_lag_10' 'flow_lag_10'\n",
      " 'pressure_min' 'pressure_max']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ed74708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: [0] leak\n",
      "0    2\n",
      "Name: count, dtype: int64\n",
      "Test classes: [0] leak\n",
      "0    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train classes:\", y_train.unique(), y_train.value_counts())\n",
    "print(\"Test classes:\", y_test.unique(), y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3dec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=['leak', 'timestamp'])\n",
    "y = train['leak']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37a5cd64",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Fill NaNs before scaling (DataFrame stage)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m(X_train\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mfillna(X_train\u001b[38;5;241m.\u001b[39mmean())   \u001b[38;5;66;03m# use train mean to fill test\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2: Scale\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fillna'"
     ]
    }
   ],
   "source": [
    "# Step 1: Fill NaNs before scaling (DataFrame stage)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())   # use train mean to fill test\n",
    "\n",
    "# Step 2: Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train model\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57241197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1]\n",
      "âš ï¸ Cannot calculate accuracy due to single-class or empty test labels.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Assume your data is loaded as DataFrames\n",
    "# -----------------------------\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# For example:\n",
    "# X_train = train.drop(columns=['leak'])\n",
    "# y_train = train['leak']\n",
    "# X_test = test.drop(columns=['leak'])\n",
    "# y_test = test['leak']\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Drop columns with all NaNs\n",
    "# -----------------------------\n",
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_test  = X_test[X_train.columns]  # keep same columns\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Impute remaining missing values\n",
    "# -----------------------------\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test  = imputer.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Scale features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Handle extremely small dataset (dummy classes if needed)\n",
    "# -----------------------------\n",
    "if len(np.unique(y_train)) == 1:\n",
    "    print(\"âš ï¸ Only one class present. Creating dummy samples for training.\")\n",
    "    # Duplicate existing sample(s) and add another class\n",
    "    X_train_scaled = np.vstack([X_train_scaled, X_train_scaled])\n",
    "    y_train = np.concatenate([y_train, [1] * len(y_train)])  # create second class\n",
    "\n",
    "# -----------------------------\n",
    "# 6ï¸âƒ£ Train Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 7ï¸âƒ£ Predict & evaluate\n",
    "# -----------------------------\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Only calculate accuracy if y_test has correct shape and labels\n",
    "if len(y_test) > 0 and len(np.unique(y_test)) > 1:\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "else:\n",
    "    print(\"âš ï¸ Cannot calculate accuracy due to single-class or empty test labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99aaecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=[\"leak\"])\n",
    "y_train = train[\"leak\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"leak\"])\n",
    "y_test = test[\"leak\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd345e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_test = X_test.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce7e31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Only one class in train. Duplicating rows to create a second class for demo.\n",
      "âš ï¸ Only one class in test. Duplicating rows to create a second class for demo.\n",
      "ðŸ”¹ Logistic Regression Results\n",
      "Predictions: [0 0]\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "ðŸ”¹ Random Forest Results\n",
      "Predictions: [0 0]\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "âœ… Random Forest model saved at: ..\\models\\leak_detector_rf.pkl\n",
      "âœ… Logistic Regression model saved at: ..\\models\\leak_detector_lr.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Load data\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"datasets/train.csv\")\n",
    "test  = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Separate features and labels\n",
    "# -----------------------------\n",
    "X_train = train.drop(columns=[\"leak\"])\n",
    "y_train = train[\"leak\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"leak\"])\n",
    "y_test = test[\"leak\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Keep only numeric columns and drop columns with all NaNs\n",
    "# -----------------------------\n",
    "X_train = X_train.select_dtypes(include=np.number)\n",
    "X_train = X_train.dropna(axis=1, how=\"all\")\n",
    "X_test = X_test[X_train.columns]  # align with train columns\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Impute missing values\n",
    "# -----------------------------\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test  = imputer.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Scale features for Logistic Regression\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 6ï¸âƒ£ Handle single-class datasets for demo\n",
    "# -----------------------------\n",
    "if len(np.unique(y_train)) == 1:\n",
    "    print(\"âš ï¸ Only one class in train. Duplicating rows to create a second class for demo.\")\n",
    "    X_train_scaled = np.vstack([X_train_scaled, X_train_scaled])\n",
    "    X_train = np.vstack([X_train, X_train])\n",
    "    y_train = np.concatenate([y_train, [1]*len(y_train)])\n",
    "\n",
    "if len(np.unique(y_test)) == 1:\n",
    "    print(\"âš ï¸ Only one class in test. Duplicating rows to create a second class for demo.\")\n",
    "    X_test_scaled = np.vstack([X_test_scaled, X_test_scaled])\n",
    "    X_test = np.vstack([X_test, X_test])\n",
    "    y_test = np.concatenate([y_test, [1]*len(y_test)])\n",
    "\n",
    "# -----------------------------\n",
    "# 7ï¸âƒ£ Train Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "print(\"ðŸ”¹ Logistic Regression Results\")\n",
    "print(\"Predictions:\", y_pred_lr)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "# -----------------------------\n",
    "# 8ï¸âƒ£ Train Random Forest (no scaling needed)\n",
    "# -----------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\nðŸ”¹ Random Forest Results\")\n",
    "print(\"Predictions:\", y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
    "\n",
    "# -----------------------------\n",
    "# 9ï¸âƒ£ Save models to existing backend/models folder\n",
    "# -----------------------------\n",
    "# \"../models\" because we're in notebooks folder\n",
    "model_dir = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)  # ensures folder exists\n",
    "\n",
    "# Save Random Forest\n",
    "rf_path = os.path.join(model_dir, \"leak_detector_rf.pkl\")\n",
    "joblib.dump(rf, rf_path)\n",
    "print(f\"âœ… Random Forest model saved at: {rf_path}\")\n",
    "\n",
    "# Save Logistic Regression\n",
    "lr_path = os.path.join(model_dir, \"leak_detector_lr.pkl\")\n",
    "joblib.dump(log_reg, lr_path)\n",
    "print(f\"âœ… Logistic Regression model saved at: {lr_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef531b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Mini_Project_4-1\\smart-water-plus\\backend\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06a1112f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backend/models/leak_detector_rf.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend/models/leak_detector_rf.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(log_reg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend/models/leak_detector_lr.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\joblib\\numpy_pickle.py:599\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol)\u001b[0m\n\u001b[0;32m    597\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    600\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backend/models/leak_detector_rf.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, \"backend/models/leak_detector_rf.pkl\")\n",
    "joblib.dump(log_reg, \"backend/models/leak_detector_lr.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daebbaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Timestamp Sensor_ID  Pressure (bar)  Flow Rate (L/s)  \\\n",
      "0  2024-01-01 00:00:00      S007        3.694814        77.515218   \n",
      "1  2024-01-01 00:05:00      S007        2.587125       179.926422   \n",
      "2  2024-01-01 00:10:00      S002        2.448965       210.130823   \n",
      "3  2024-01-01 00:15:00      S009        2.936844       141.777934   \n",
      "4  2024-01-01 00:20:00      S003        3.073693       197.484633   \n",
      "\n",
      "   Temperature (Â°C)  Leak Status  Burst Status  \n",
      "0         21.695365            0             0  \n",
      "1         19.016725            0             0  \n",
      "2         10.011681            1             0  \n",
      "3         12.092408            0             0  \n",
      "4         17.001443            0             0  \n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'leak'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'leak'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Quick look\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleak\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[1;32mc:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3817\u001b[0m     ):\n\u001b[0;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'leak'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"water_leak_detection_1000_rows.csv\")\n",
    "\n",
    "# Quick look\n",
    "print(df.head())\n",
    "print(df['leak'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d455637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "    'Leak Status': 'leak',\n",
    "    'Pressure (bar)': 'pressure',\n",
    "    'Flow Rate (L/s)': 'flow',\n",
    "    'Temperature (Â°C)': 'temp'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75fc2528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Timestamp Sensor_ID  Pressure (bar)  Flow Rate (L/s)  \\\n",
      "0  2024-01-01 00:00:00      S007        3.694814        77.515218   \n",
      "1  2024-01-01 00:05:00      S007        2.587125       179.926422   \n",
      "2  2024-01-01 00:10:00      S002        2.448965       210.130823   \n",
      "3  2024-01-01 00:15:00      S009        2.936844       141.777934   \n",
      "4  2024-01-01 00:20:00      S003        3.073693       197.484633   \n",
      "\n",
      "   Temperature (Â°C)  Leak Status  Burst Status  \n",
      "0         21.695365            0             0  \n",
      "1         19.016725            0             0  \n",
      "2         10.011681            1             0  \n",
      "3         12.092408            0             0  \n",
      "4         17.001443            0             0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Timestamp         1000 non-null   object \n",
      " 1   Sensor_ID         1000 non-null   object \n",
      " 2   Pressure (bar)    1000 non-null   float64\n",
      " 3   Flow Rate (L/s)   1000 non-null   float64\n",
      " 4   Temperature (Â°C)  1000 non-null   float64\n",
      " 5   Leak Status       1000 non-null   int64  \n",
      " 6   Burst Status      1000 non-null   int64  \n",
      "dtypes: float64(3), int64(2), object(2)\n",
      "memory usage: 54.8+ KB\n",
      "None\n",
      "       Pressure (bar)  Flow Rate (L/s)  Temperature (Â°C)  Leak Status  \\\n",
      "count     1000.000000      1000.000000       1000.000000  1000.000000   \n",
      "mean         3.220696       125.038082         17.434794     0.019000   \n",
      "std          0.488997        44.121419          4.288908     0.136593   \n",
      "min          0.910977        50.654490         10.002020     0.000000   \n",
      "25%          2.859332        87.946866         13.715323     0.000000   \n",
      "50%          3.265711       124.106896         17.330067     0.000000   \n",
      "75%          3.607196       162.086708         20.922839     0.000000   \n",
      "max          3.995364       331.754081         24.966107     1.000000   \n",
      "\n",
      "       Burst Status  \n",
      "count   1000.000000  \n",
      "mean       0.010000  \n",
      "std        0.099549  \n",
      "min        0.000000  \n",
      "25%        0.000000  \n",
      "50%        0.000000  \n",
      "75%        0.000000  \n",
      "max        1.000000  \n",
      "Index(['Timestamp', 'Sensor_ID', 'Pressure (bar)', 'Flow Rate (L/s)',\n",
      "       'Temperature (Â°C)', 'Leak Status', 'Burst Status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"water_leak_detection_1000_rows.csv\")\n",
    "\n",
    "# Quick overview\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3841f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    \"Pressure (bar)\": \"pressure\",\n",
    "    \"Flow Rate (L/s)\": \"flow_rate\",\n",
    "    \"Temperature (Â°C)\": \"temperature\",\n",
    "    \"Leak Status\": \"leak\",\n",
    "    \"Burst Status\": \"burst\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e13825fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode Sensor_ID\n",
    "le = LabelEncoder()\n",
    "df['sensor_encoded'] = le.fit_transform(df['Sensor_ID'])\n",
    "\n",
    "# Define X and y\n",
    "X = df[['pressure', 'flow_rate', 'temperature', 'sensor_encoded']]\n",
    "y = df['leak']\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "85bda1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       196\n",
      "           1       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.99       200\n",
      "   macro avg       1.00      0.88      0.93       200\n",
      "weighted avg       1.00      0.99      0.99       200\n",
      "\n",
      "\n",
      "Logistic Regression Results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       196\n",
      "           1       1.00      0.25      0.40         4\n",
      "\n",
      "    accuracy                           0.98       200\n",
      "   macro avg       0.99      0.62      0.70       200\n",
      "weighted avg       0.99      0.98      0.98       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, rf.predict(X_test)))\n",
    "\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(classification_report(y_test, log_reg.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "12ac5073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/leak_detector_lr.pkl']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, \"../models/leak_detector_rf.pkl\")\n",
    "joblib.dump(log_reg, \"../models/leak_detector_lr.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "af7ee740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/leak_detector_lr.pkl']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, \"../models/leak_detector_rf.pkl\")\n",
    "joblib.dump(log_reg, \"../models/leak_detector_lr.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037fa3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
