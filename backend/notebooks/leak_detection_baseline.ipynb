{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59a42054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d09940ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(r\"C:\\Mini_Project_4-1\\smart-water-plus\\simulation\\simulation\\datasets\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Mini_Project_4-1\\smart-water-plus\\simulation\\simulation\\datasets\\test.csv\")\n",
    "\n",
    "\n",
    "X_train, y_train = train.drop(columns=['leak', 'timestamp']), train['leak']\n",
    "X_test, y_test = test.drop(columns=['leak', 'timestamp']), test['leak']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d7eef058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'pressure', 'flow', 'rolling_mean', 'delta',\n",
      "       'pressure_lag_1', 'flow_lag_1', 'pressure_lag_2', 'flow_lag_2',\n",
      "       'pressure_lag_3', 'flow_lag_3', 'pressure_lag_4', 'flow_lag_4',\n",
      "       'pressure_lag_5', 'flow_lag_5', 'pressure_lag_6', 'flow_lag_6',\n",
      "       'pressure_lag_7', 'flow_lag_7', 'pressure_lag_8', 'flow_lag_8',\n",
      "       'pressure_lag_9', 'flow_lag_9', 'pressure_lag_10', 'flow_lag_10',\n",
      "       'pressure_min', 'pressure_max', 'pressure_std', 'flow_min', 'flow_max',\n",
      "       'flow_std', 'leak'],\n",
      "      dtype='object')\n",
      "Index(['timestamp', 'pressure', 'flow', 'rolling_mean', 'delta',\n",
      "       'pressure_lag_1', 'flow_lag_1', 'pressure_lag_2', 'flow_lag_2',\n",
      "       'pressure_lag_3', 'flow_lag_3', 'pressure_lag_4', 'flow_lag_4',\n",
      "       'pressure_lag_5', 'flow_lag_5', 'pressure_lag_6', 'flow_lag_6',\n",
      "       'pressure_lag_7', 'flow_lag_7', 'pressure_lag_8', 'flow_lag_8',\n",
      "       'pressure_lag_9', 'flow_lag_9', 'pressure_lag_10', 'flow_lag_10',\n",
      "       'pressure_min', 'pressure_max', 'pressure_std', 'flow_min', 'flow_max',\n",
      "       'flow_std', 'leak'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(train.columns)\n",
    "print(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b74c8942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1101: RuntimeWarning: invalid value encountered in divide\n",
      "  updated_mean = (last_sum + new_sum) / updated_sample_count\n",
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1106: RuntimeWarning: invalid value encountered in divide\n",
      "  T = new_sum / new_sample_count\n",
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\utils\\extmath.py:1126: RuntimeWarning: invalid value encountered in divide\n",
      "  new_unnormalized_variance -= correction**2 / new_sample_count\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9d816a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pressure           2\n",
      "flow               0\n",
      "rolling_mean       2\n",
      "delta              0\n",
      "pressure_lag_1     2\n",
      "flow_lag_1         0\n",
      "pressure_lag_2     2\n",
      "flow_lag_2         0\n",
      "pressure_lag_3     2\n",
      "flow_lag_3         2\n",
      "pressure_lag_4     2\n",
      "flow_lag_4         2\n",
      "pressure_lag_5     2\n",
      "flow_lag_5         2\n",
      "pressure_lag_6     2\n",
      "flow_lag_6         2\n",
      "pressure_lag_7     2\n",
      "flow_lag_7         2\n",
      "pressure_lag_8     2\n",
      "flow_lag_8         2\n",
      "pressure_lag_9     2\n",
      "flow_lag_9         2\n",
      "pressure_lag_10    2\n",
      "flow_lag_10        2\n",
      "pressure_min       2\n",
      "pressure_max       2\n",
      "pressure_std       0\n",
      "flow_min           0\n",
      "flow_max           0\n",
      "flow_std           0\n",
      "dtype: int64\n",
      "pressure           0\n",
      "flow               0\n",
      "rolling_mean       0\n",
      "delta              0\n",
      "pressure_lag_1     1\n",
      "flow_lag_1         0\n",
      "pressure_lag_2     1\n",
      "flow_lag_2         0\n",
      "pressure_lag_3     1\n",
      "flow_lag_3         1\n",
      "pressure_lag_4     1\n",
      "flow_lag_4         1\n",
      "pressure_lag_5     1\n",
      "flow_lag_5         1\n",
      "pressure_lag_6     1\n",
      "flow_lag_6         1\n",
      "pressure_lag_7     1\n",
      "flow_lag_7         1\n",
      "pressure_lag_8     1\n",
      "flow_lag_8         1\n",
      "pressure_lag_9     1\n",
      "flow_lag_9         1\n",
      "pressure_lag_10    1\n",
      "flow_lag_10        1\n",
      "pressure_min       0\n",
      "pressure_max       0\n",
      "pressure_std       0\n",
      "flow_min           0\n",
      "flow_max           0\n",
      "flow_std           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.isnull().sum())\n",
    "print(X_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3de0a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['pressure' 'rolling_mean' 'pressure_lag_1' 'pressure_lag_2'\n",
      " 'pressure_lag_3' 'flow_lag_3' 'pressure_lag_4' 'flow_lag_4'\n",
      " 'pressure_lag_5' 'flow_lag_5' 'pressure_lag_6' 'flow_lag_6'\n",
      " 'pressure_lag_7' 'flow_lag_7' 'pressure_lag_8' 'flow_lag_8'\n",
      " 'pressure_lag_9' 'flow_lag_9' 'pressure_lag_10' 'flow_lag_10'\n",
      " 'pressure_min' 'pressure_max']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n",
      "c:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\sklearn\\impute\\_base.py:635: UserWarning: Skipping features without any observed values: ['pressure' 'rolling_mean' 'pressure_lag_1' 'pressure_lag_2'\n",
      " 'pressure_lag_3' 'flow_lag_3' 'pressure_lag_4' 'flow_lag_4'\n",
      " 'pressure_lag_5' 'flow_lag_5' 'pressure_lag_6' 'flow_lag_6'\n",
      " 'pressure_lag_7' 'flow_lag_7' 'pressure_lag_8' 'flow_lag_8'\n",
      " 'pressure_lag_9' 'flow_lag_9' 'pressure_lag_10' 'flow_lag_10'\n",
      " 'pressure_min' 'pressure_max']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ed74708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train classes: [0] leak\n",
      "0    2\n",
      "Name: count, dtype: int64\n",
      "Test classes: [0] leak\n",
      "0    1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train classes:\", y_train.unique(), y_train.value_counts())\n",
    "print(\"Test classes:\", y_test.unique(), y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3dec36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(columns=['leak', 'timestamp'])\n",
    "y = train['leak']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37a5cd64",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'fillna'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Fill NaNs before scaling (DataFrame stage)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfillna\u001b[49m(X_train\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m      3\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mfillna(X_train\u001b[38;5;241m.\u001b[39mmean())   \u001b[38;5;66;03m# use train mean to fill test\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Step 2: Scale\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'fillna'"
     ]
    }
   ],
   "source": [
    "# Step 1: Fill NaNs before scaling (DataFrame stage)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())   # use train mean to fill test\n",
    "\n",
    "# Step 2: Scale\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Train model\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "57241197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1]\n",
      "âš ï¸ Cannot calculate accuracy due to single-class or empty test labels.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Assume your data is loaded as DataFrames\n",
    "# -----------------------------\n",
    "# X_train, X_test, y_train, y_test\n",
    "\n",
    "# For example:\n",
    "# X_train = train.drop(columns=['leak'])\n",
    "# y_train = train['leak']\n",
    "# X_test = test.drop(columns=['leak'])\n",
    "# y_test = test['leak']\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Drop columns with all NaNs\n",
    "# -----------------------------\n",
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_test  = X_test[X_train.columns]  # keep same columns\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Impute remaining missing values\n",
    "# -----------------------------\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test  = imputer.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Scale features\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Handle extremely small dataset (dummy classes if needed)\n",
    "# -----------------------------\n",
    "if len(np.unique(y_train)) == 1:\n",
    "    print(\"âš ï¸ Only one class present. Creating dummy samples for training.\")\n",
    "    # Duplicate existing sample(s) and add another class\n",
    "    X_train_scaled = np.vstack([X_train_scaled, X_train_scaled])\n",
    "    y_train = np.concatenate([y_train, [1] * len(y_train)])  # create second class\n",
    "\n",
    "# -----------------------------\n",
    "# 6ï¸âƒ£ Train Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# -----------------------------\n",
    "# 7ï¸âƒ£ Predict & evaluate\n",
    "# -----------------------------\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Only calculate accuracy if y_test has correct shape and labels\n",
    "if len(y_test) > 0 and len(np.unique(y_test)) > 1:\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", acc)\n",
    "else:\n",
    "    print(\"âš ï¸ Cannot calculate accuracy due to single-class or empty test labels.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99aaecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=[\"leak\"])\n",
    "y_train = train[\"leak\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"leak\"])\n",
    "y_test = test[\"leak\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd345e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.dropna(axis=1, how='all')\n",
    "X_test = X_test.dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ce7e31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Only one class in train. Duplicating rows to create a second class for demo.\n",
      "âš ï¸ Only one class in test. Duplicating rows to create a second class for demo.\n",
      "ðŸ”¹ Logistic Regression Results\n",
      "Predictions: [0 0]\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "\n",
      "ðŸ”¹ Random Forest Results\n",
      "Predictions: [0 0]\n",
      "Accuracy: 0.5\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      1.00      0.67         1\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n",
      "âœ… Random Forest model saved at: ..\\models\\leak_detector_rf.pkl\n",
      "âœ… Logistic Regression model saved at: ..\\models\\leak_detector_lr.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Load data\n",
    "# -----------------------------\n",
    "train = pd.read_csv(\"datasets/train.csv\")\n",
    "test  = pd.read_csv(\"datasets/test.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Separate features and labels\n",
    "# -----------------------------\n",
    "X_train = train.drop(columns=[\"leak\"])\n",
    "y_train = train[\"leak\"]\n",
    "\n",
    "X_test = test.drop(columns=[\"leak\"])\n",
    "y_test = test[\"leak\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Keep only numeric columns and drop columns with all NaNs\n",
    "# -----------------------------\n",
    "X_train = X_train.select_dtypes(include=np.number)\n",
    "X_train = X_train.dropna(axis=1, how=\"all\")\n",
    "X_test = X_test[X_train.columns]  # align with train columns\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ Impute missing values\n",
    "# -----------------------------\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test  = imputer.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Scale features for Logistic Regression\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n",
    "# -----------------------------\n",
    "# 6ï¸âƒ£ Handle single-class datasets for demo\n",
    "# -----------------------------\n",
    "if len(np.unique(y_train)) == 1:\n",
    "    print(\"âš ï¸ Only one class in train. Duplicating rows to create a second class for demo.\")\n",
    "    X_train_scaled = np.vstack([X_train_scaled, X_train_scaled])\n",
    "    X_train = np.vstack([X_train, X_train])\n",
    "    y_train = np.concatenate([y_train, [1]*len(y_train)])\n",
    "\n",
    "if len(np.unique(y_test)) == 1:\n",
    "    print(\"âš ï¸ Only one class in test. Duplicating rows to create a second class for demo.\")\n",
    "    X_test_scaled = np.vstack([X_test_scaled, X_test_scaled])\n",
    "    X_test = np.vstack([X_test, X_test])\n",
    "    y_test = np.concatenate([y_test, [1]*len(y_test)])\n",
    "\n",
    "# -----------------------------\n",
    "# 7ï¸âƒ£ Train Logistic Regression\n",
    "# -----------------------------\n",
    "log_reg = LogisticRegression(max_iter=500)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_scaled)\n",
    "print(\"ðŸ”¹ Logistic Regression Results\")\n",
    "print(\"Predictions:\", y_pred_lr)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(classification_report(y_test, y_pred_lr, zero_division=0))\n",
    "\n",
    "# -----------------------------\n",
    "# 8ï¸âƒ£ Train Random Forest (no scaling needed)\n",
    "# -----------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "print(\"\\nðŸ”¹ Random Forest Results\")\n",
    "print(\"Predictions:\", y_pred_rf)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
    "\n",
    "# -----------------------------\n",
    "# 9ï¸âƒ£ Save models to existing backend/models folder\n",
    "# -----------------------------\n",
    "# \"../models\" because we're in notebooks folder\n",
    "model_dir = os.path.join(\"..\", \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)  # ensures folder exists\n",
    "\n",
    "# Save Random Forest\n",
    "rf_path = os.path.join(model_dir, \"leak_detector_rf.pkl\")\n",
    "joblib.dump(rf, rf_path)\n",
    "print(f\"âœ… Random Forest model saved at: {rf_path}\")\n",
    "\n",
    "# Save Logistic Regression\n",
    "lr_path = os.path.join(model_dir, \"leak_detector_lr.pkl\")\n",
    "joblib.dump(log_reg, lr_path)\n",
    "print(f\"âœ… Logistic Regression model saved at: {lr_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ef531b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Mini_Project_4-1\\smart-water-plus\\backend\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06a1112f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'backend/models/leak_detector_rf.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjoblib\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend/models/leak_detector_rf.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(log_reg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackend/models/leak_detector_lr.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Mini_Project_4-1\\smart-water-plus\\venv\\lib\\site-packages\\joblib\\numpy_pickle.py:599\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(value, filename, compress, protocol)\u001b[0m\n\u001b[0;32m    597\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    600\u001b[0m         NumpyPickler(f, protocol\u001b[38;5;241m=\u001b[39mprotocol)\u001b[38;5;241m.\u001b[39mdump(value)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'backend/models/leak_detector_rf.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(rf, \"backend/models/leak_detector_rf.pkl\")\n",
    "joblib.dump(log_reg, \"backend/models/leak_detector_lr.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebbaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
